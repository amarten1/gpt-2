{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "gpt-2.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/amarten1/gpt-2/blob/master/gpt_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n6m4azWJq7yA",
        "colab_type": "text"
      },
      "source": [
        "Clone git"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EjyF3LwTq3cr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "outputId": "d1a83ebe-37ed-420d-e0aa-307ecf422d11"
      },
      "source": [
        "!git clone https://github.com/amarten1/gpt-2.git"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'gpt-2'...\n",
            "remote: Enumerating objects: 218, done.\u001b[K\n",
            "remote: Total 218 (delta 0), reused 0 (delta 0), pack-reused 218\u001b[K\n",
            "Receiving objects: 100% (218/218), 4.37 MiB | 6.14 MiB/s, done.\n",
            "Resolving deltas: 100% (117/117), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "80ezXltFrji0",
        "colab_type": "text"
      },
      "source": [
        "Download model\n",
        "1558M for 1.5 Billion dataset,\n",
        "124M,\n",
        "355M,\n",
        "774M,"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pX2iPRcxrmO1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "outputId": "4a1ecbf3-f3aa-4bd5-bcfd-a8dc49d15a81"
      },
      "source": [
        "!python gpt-2/download_model.py 1558M"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\rFetching checkpoint:   0%|                                              | 0.00/77.0 [00:00<?, ?it/s]\rFetching checkpoint: 1.00kit [00:00, 837kit/s]                                                      \n",
            "Fetching encoder.json: 1.04Mit [00:00, 39.0Mit/s]                                                   \n",
            "Fetching hparams.json: 1.00kit [00:00, 654kit/s]                                                    \n",
            "Fetching model.ckpt.data-00000-of-00001: 6.23Git [02:02, 51.1Mit/s]                                 \n",
            "Fetching model.ckpt.index: 21.0kit [00:00, 13.5Mit/s]                                               \n",
            "Fetching model.ckpt.meta: 1.84Mit [00:00, 45.6Mit/s]                                                \n",
            "Fetching vocab.bpe: 457kit [00:00, 37.5Mit/s]                                                       \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NSqIi320I4gm",
        "colab_type": "text"
      },
      "source": [
        "GPT-2 Colab"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YE6_Bfsv5kCe",
        "colab_type": "code",
        "outputId": "2ca2fb68-0838-4236-f9b5-3043a7de440c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 627
        }
      },
      "source": [
        "!pip install -r gpt-2/requirements.txt"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting fire>=0.1.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d9/69/faeaae8687f4de0f5973694d02e9d6c3eb827636a009157352d98de1129e/fire-0.2.1.tar.gz (76kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 4.7MB/s \n",
            "\u001b[?25hCollecting regex==2017.4.5\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/36/62/c0c0d762ffd4ffaf39f372eb8561b8d491a11ace5a7884610424a8b40f95/regex-2017.04.05.tar.gz (601kB)\n",
            "\u001b[K     |████████████████████████████████| 604kB 19.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests==2.21.0 in /usr/local/lib/python3.6/dist-packages (from -r gpt-2/requirements.txt (line 3)) (2.21.0)\n",
            "Collecting tqdm==4.31.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6c/4b/c38b5144cf167c4f52288517436ccafefe9dc01b8d1c190e18a6b154cd4a/tqdm-4.31.1-py2.py3-none-any.whl (48kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 6.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from fire>=0.1.3->-r gpt-2/requirements.txt (line 1)) (1.12.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.6/dist-packages (from fire>=0.1.3->-r gpt-2/requirements.txt (line 1)) (1.1.0)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests==2.21.0->-r gpt-2/requirements.txt (line 3)) (3.0.4)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests==2.21.0->-r gpt-2/requirements.txt (line 3)) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests==2.21.0->-r gpt-2/requirements.txt (line 3)) (2019.9.11)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests==2.21.0->-r gpt-2/requirements.txt (line 3)) (2.8)\n",
            "Building wheels for collected packages: fire, regex\n",
            "  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fire: filename=fire-0.2.1-py2.py3-none-any.whl size=103527 sha256=7c25ebfd025ff32731ede1436bd691f9c80fc65f772e8a2848f64625d9e85a97\n",
            "  Stored in directory: /root/.cache/pip/wheels/31/9c/c0/07b6dc7faf1844bb4688f46b569efe6cafaa2179c95db821da\n",
            "  Building wheel for regex (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for regex: filename=regex-2017.4.5-cp36-cp36m-linux_x86_64.whl size=533171 sha256=f0242fd8a268e10a819f3cf9822378dedce0ba19bda944ce062fbe4a36dfa39b\n",
            "  Stored in directory: /root/.cache/pip/wheels/75/07/38/3c16b529d50cb4e0cd3dbc7b75cece8a09c132692c74450b01\n",
            "Successfully built fire regex\n",
            "Installing collected packages: fire, regex, tqdm\n",
            "  Found existing installation: tqdm 4.28.1\n",
            "    Uninstalling tqdm-4.28.1:\n",
            "      Successfully uninstalled tqdm-4.28.1\n",
            "Successfully installed fire-0.2.1 regex-2017.4.5 tqdm-4.31.1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "tqdm"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NPH3u0Ic4YjG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#model parameters\n",
        "MODEL_NAME='1558M'\n",
        "\n",
        "#seed none for ramdom, set for repeatability\n",
        "SEED=None\n",
        "\n",
        "#number of samples to return and batch size batch must be divisible by nsamples\n",
        "NSAMPLES=1\n",
        "BATCH_SIZE=1\n",
        "\n",
        "#length of returned text, None default\n",
        "LENGTH=None\n",
        "\n",
        "#temperature is randomness 0 => repetative, higher more random, default 1\n",
        "TEMPERATURE=1\n",
        "\n",
        "#top k is numberof words considered in each step, 0 is special case for no restricion\n",
        "TOP_K=40\n",
        "\n",
        "TOP_P=1\n",
        "MODELS_DIR='models'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UB0RQbwv5TEw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "2eed796f-e16d-42b1-c094-5746f1b23e55"
      },
      "source": [
        "!python gpt-2/src/interactive_conditional_samples.py {MODEL_NAME} {SEED} {NSAMPLES} {BATCH_SIZE} \\\n",
        "  {LENGTH} {TEMPERATURE} {TOP_K} {TOP_P} {MODELS_DIR}"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From gpt-2/src/interactive_conditional_samples.py:57: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "2019-11-17 20:39:59.374578: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\n",
            "2019-11-17 20:39:59.400866: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-11-17 20:39:59.401697: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: \n",
            "name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\n",
            "pciBusID: 0000:00:04.0\n",
            "2019-11-17 20:39:59.404673: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0\n",
            "2019-11-17 20:39:59.406380: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0\n",
            "2019-11-17 20:39:59.410627: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0\n",
            "2019-11-17 20:39:59.417303: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0\n",
            "2019-11-17 20:39:59.427687: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0\n",
            "2019-11-17 20:39:59.434423: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0\n",
            "2019-11-17 20:39:59.448577: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
            "2019-11-17 20:39:59.448824: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-11-17 20:39:59.449726: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-11-17 20:39:59.450388: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0\n",
            "2019-11-17 20:39:59.456520: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz\n",
            "2019-11-17 20:39:59.456798: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x256d480 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
            "2019-11-17 20:39:59.456834: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
            "2019-11-17 20:39:59.515842: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-11-17 20:39:59.516759: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x256d640 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "2019-11-17 20:39:59.516791: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla K80, Compute Capability 3.7\n",
            "2019-11-17 20:39:59.516986: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-11-17 20:39:59.517714: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: \n",
            "name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\n",
            "pciBusID: 0000:00:04.0\n",
            "2019-11-17 20:39:59.517803: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0\n",
            "2019-11-17 20:39:59.517831: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0\n",
            "2019-11-17 20:39:59.517864: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0\n",
            "2019-11-17 20:39:59.517887: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0\n",
            "2019-11-17 20:39:59.517908: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0\n",
            "2019-11-17 20:39:59.517933: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0\n",
            "2019-11-17 20:39:59.517959: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
            "2019-11-17 20:39:59.518057: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-11-17 20:39:59.518808: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-11-17 20:39:59.519466: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0\n",
            "2019-11-17 20:39:59.519574: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0\n",
            "2019-11-17 20:39:59.521788: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2019-11-17 20:39:59.521824: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 \n",
            "2019-11-17 20:39:59.521838: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N \n",
            "2019-11-17 20:39:59.521995: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-11-17 20:39:59.522705: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-11-17 20:39:59.523387: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "2019-11-17 20:39:59.523442: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10805 MB memory) -> physical GPU (device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7)\n",
            "WARNING:tensorflow:From gpt-2/src/interactive_conditional_samples.py:58: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From gpt-2/src/interactive_conditional_samples.py:60: The name tf.set_random_seed is deprecated. Please use tf.compat.v1.set_random_seed instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/gpt-2/src/sample.py:51: The name tf.AUTO_REUSE is deprecated. Please use tf.compat.v1.AUTO_REUSE instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/gpt-2/src/model.py:148: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/gpt-2/src/model.py:152: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/gpt-2/src/model.py:36: The name tf.rsqrt is deprecated. Please use tf.math.rsqrt instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/gpt-2/src/sample.py:64: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.cast` instead.\n",
            "WARNING:tensorflow:From /content/gpt-2/src/sample.py:16: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /content/gpt-2/src/sample.py:67: multinomial (from tensorflow.python.ops.random_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.random.categorical` instead.\n",
            "WARNING:tensorflow:From gpt-2/src/interactive_conditional_samples.py:68: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n",
            "\n",
            "Model prompt >>> Today\n",
            "2019-11-17 20:41:35.248510: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0\n",
            "======================================== SAMPLE 1 ========================================\n",
            ", you can't really do that; you can't find that information in the database,\" says Richard Pfefferl, a professor of biostatistics at the Albert Einstein College of Medicine and co-director of the Center for Research on Health and Aging.\n",
            "\n",
            "So what's a researcher to do? Pfefferl had an idea — he thought he could collect a different kind of information from the same patient — and he set about making the idea a reality. His first step? Start building the first ever DNA-sequencing technology to study aging.\n",
            "\n",
            "The process started in 2011 inside The Scripps Research Institute (TSRI) as Pfefferl, his colleagues at University of Pennsylvania and the University of Iowa, and a large team of biotech startup companies tried to solve what had always been a persistent problem — how to get at the most important part of aging to understand what caused it.\n",
            "\n",
            "Until now, researchers had to guess what age you are based on your parents' age. But this was just guessing, and even for the most reliable results, there was a certain chance that a person's age could be wrong. That made genetic sequencing a tricky proposition. But TTI was able to recruit hundreds of individuals with the same type of genetic mutations as those who'd become ill during the study, so that it had a pool of people who might have a \"healthy life span\" — and therefore, their genetic material could be mapped by the sequencers. As the scientists tested the mutations to find the ones most likely to cause ill health and death, they discovered an interesting pattern.\n",
            "\n",
            "\"We found a pattern by looking at the genes that were linked to more-advanced age,\" Pfefferl says.\n",
            "\n",
            "The \"most-advanced age\" they identified is not exactly the same as the oldest person around, but it's close enough that we can all look at it like this.\n",
            "\n",
            "But there's more. To get their results, the researchers needed data from about 70 people who had participated in a much smaller clinical trial called ALTIM. The ALTIM studies involve enrolling patients diagnosed with either type 2 diabetes, an insulin dependent condition, or high blood pressure, with treatment aimed at lowering blood sugar levels. But some people with high blood pressure who were in good health at the outset of the trials were diagnosed with heart attacks six years earlier, and their blood sugar levels were getting worse. So the trial doctors switched out some of the patients whose blood sugar readings were getting\n",
            "================================================================================\n",
            "Model prompt >>> One fish, two fish, red fish, blue fish\n",
            "======================================== SAMPLE 1 ========================================\n",
            ", red fish, blue fish and so on...\n",
            "\n",
            "Pineapple: The story of what happens when two people can talk to fish is one that's just incredible, even for a Hollywood director...\n",
            "\n",
            "Miles: We've seen Pineapple go on, but what I love about this movie is the way the story can be told. There are multiple versions and even a whole world outside of the island, where things keep going on.\n",
            "\n",
            "Miles: It's a little hard to do what I did on Pineapple, and you have to keep the island a little bit separate at times to do that. We're trying to get more and more in this one episode a parallel world within the island, kind of a whole different take on the story.\n",
            "\n",
            "Pineapple (Season 3, Episode 7-10) : How the writers were able to keep the audience so engaged with this story is incredible. The fact that in less than a week's time, they were able to produce three separate episodes that the audience really connected with was a great surprise.\n",
            "\n",
            "Miles: It didn't happen overnight. It was the first time they did anything like this. It takes a very long time to create an island and you want people to be invested in the story, and they were, and they kept going.\n",
            "\n",
            "How much do they want to tell this in four seasons?\n",
            "\n",
            "Miles: They are saying this is the end; they really have no qualms about saying this is it! But it's more for them to make sure the show is good enough. They have a couple of ideas on the table, but at this point they just want to make sure they get it right on TV.\n",
            "\n",
            "Pineapple (Season 3, Episode 9) , episode 9, \"The Fates of the Fingers\" : It's a fun ride to go see a little island and then go see how this world ends...\n",
            "\n",
            "Miles: ...or ends and then come to an inescapable conclusion, which is always fun. That's why I say it's more for them. They want to make sure it works.\n",
            "\n",
            "Pineapple (Season 3, Episode 10) : That's the challenge... how do you tell both of them at once?\n",
            "\n",
            "Miles: They have some episodes where they are playing the big finale and then the one that follows it is more small world like, or kind of the same characters in different parts of the island...\n",
            "\n",
            "Pine\n",
            "================================================================================\n",
            "Model prompt >>> In my chamber I heard a tapping\n",
            "======================================== SAMPLE 1 ========================================\n",
            " sound. I went to the door and examined it. There was a little girl sitting on the chair in front of the door. She was very much frightened.\n",
            "\n",
            "When she had noticed me, she said, \"I have never seen a man who had more hair than you.\"\n",
            "\n",
            "I told her that my hair would give me an advantage whenever we played.\n",
            "\n",
            "She became very silent. Then he came forward.\n",
            "\n",
            "He had a beautiful smile. He said, in broken English, \"You are the one who told me that I had to do good deeds in my country.\"\n",
            "\n",
            "When I heard these words, I felt happy like I had been born again. Suddenly I remembered that I had a new spirit.\n",
            "\n",
            "\n",
            "I told him who I was. He said that I was the spirit of justice, prosperity, and success.\n",
            "\n",
            "I told him that I had no need to go where I was and where I lived; that I had my own kingdom in the heavens, and did not need to go anywhere.\n",
            "\n",
            "I told him that I was with my dear friends who would give me guidance. When he asked me what I wished, I promised him that I would go to the other two disciples in the other country.\n",
            "\n",
            "We went in the carriage.\n",
            "\n",
            "I sat beside the carriage horse and, when the carriage stopped, I went to him. His feet were bare, and I knew where the grass was growing because it looked exactly like my own.\n",
            "\n",
            "I asked him why he had such beautiful legs. He told me that his mother had made them in a garden on high land. \"All girls have legs like this,\" he said.\n",
            "\n",
            "When we arrived at the other two places, I was not allowed. I told the other friends what had happened.\n",
            "\n",
            "The other disciples were very joyful over our discovery.\n",
            "\n",
            "When I began to grow my hair, my disciples told me that I was too beautiful to grow my hair.\n",
            "\n",
            "I answered, \"My hair means nothing to you.\" I thought that their spiritual values and desires were superior to my physical limitations and needs. It was important for me to have my hair.\n",
            "\n",
            "After the discovery of the other disciples, I stayed with them in Europe for eight years. During that time I received instruction from them. They taught me that I would be happy in my new life.\n",
            "\n",
            "I saw that there were many different forms of existence, different ways of happiness, even love.\n",
            "\n",
            "There were men who were\n",
            "================================================================================\n",
            "Model prompt >>> Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.6/contextlib.py\", line 99, in __exit__\n",
            "    self.gen.throw(type, value, traceback)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py\", line 5480, in get_controller\n",
            "    yield g\n",
            "  File \"gpt-2/src/interactive_conditional_samples.py\", line 73, in interact_model\n",
            "    raw_text = input(\"Model prompt >>> \")\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"gpt-2/src/interactive_conditional_samples.py\", line 91, in <module>\n",
            "    fire.Fire(interact_model)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/fire/core.py\", line 138, in Fire\n",
            "    component_trace = _Fire(component, args, parsed_flag_args, context, name)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/fire/core.py\", line 471, in _Fire\n",
            "    target=component.__name__)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/fire/core.py\", line 675, in _CallAndUpdateTrace\n",
            "    component = fn(*varargs, **kwargs)\n",
            "  File \"gpt-2/src/interactive_conditional_samples.py\", line 88, in interact_model\n",
            "    print(\"=\" * 80)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\", line 1634, in __exit__\n",
            "    close_thread.join(30.0)\n",
            "  File \"/usr/lib/python3.6/threading.py\", line 1060, in join\n",
            "    self._wait_for_tstate_lock(timeout=max(timeout, 0))\n",
            "  File \"/usr/lib/python3.6/threading.py\", line 1072, in _wait_for_tstate_lock\n",
            "    elif lock.acquire(block, timeout):\n",
            "KeyboardInterrupt\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}