{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "gpt-2.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/amarten1/gpt-2/blob/master/gpt_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n6m4azWJq7yA",
        "colab_type": "text"
      },
      "source": [
        "Clone git"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EjyF3LwTq3cr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!git clone https://github.com/amarten1/gpt-2.git"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "80ezXltFrji0",
        "colab_type": "text"
      },
      "source": [
        "Models available\n",
        "\n",
        "*   1558M\n",
        "*   774M\n",
        "*   355M\n",
        "*   124M"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NPH3u0Ic4YjG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#model parameters\n",
        "MODEL_NAME='1558M'\n",
        "\n",
        "#seed none for ramdom, set for repeatability\n",
        "SEED=None\n",
        "\n",
        "#number of samples to return and batch size batch must be divisible by nsamples\n",
        "NSAMPLES=1\n",
        "BATCH_SIZE=1\n",
        "\n",
        "#length of returned text, None default\n",
        "LENGTH=1000\n",
        "\n",
        "#temperature is randomness 0 => repetative, higher more random, default 1\n",
        "TEMPERATURE=1\n",
        "\n",
        "#top k is numberof words considered in each step, 0 is special case for no restricion\n",
        "TOP_K=40\n",
        "\n",
        "TOP_P=1\n",
        "MODELS_DIR='models'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pX2iPRcxrmO1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!python gpt-2/download_model.py {MODEL_NAME}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NSqIi320I4gm",
        "colab_type": "text"
      },
      "source": [
        "GPT-2 Colab"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YE6_Bfsv5kCe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install -r gpt-2/requirements.txt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A3fsS5iiS_Xn",
        "colab_type": "text"
      },
      "source": [
        "To run with input"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UB0RQbwv5TEw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!python gpt-2/src/interactive_conditional_samples.py {MODEL_NAME} {SEED} {NSAMPLES} {BATCH_SIZE} \\\n",
        "  {LENGTH} {TEMPERATURE} {TOP_K} {TOP_P} {MODELS_DIR}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l6TolHa1TFOV",
        "colab_type": "text"
      },
      "source": [
        "To run without input"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ut-QHDtTHn8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!python gpt-2/src/generate_unconditional_samples.py {MODEL_NAME} {SEED} {NSAMPLES} {BATCH_SIZE} \\\n",
        "  {LENGTH} {TEMPERATURE} {TOP_K} {TOP_P} {MODELS_DIR}"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}